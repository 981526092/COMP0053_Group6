{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook investigates whether using flattened data after sliding window segmentation with random forest and Adaboost classifiers produces better results compared to directly using frame-by-frame data for model training.\n",
    "Additionally, this notebook explores if downsampling leads to improved model performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import mode\n",
    "from tqdm import tqdm\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_path = \"CoordinateData\\\\train\"\n",
    "validation_path = \"CoordinateData\\\\validation\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def segment_data(data: np.ndarray, window_size: int = 180, overlap_ratio: float = 0.75, by_type: bool = True, min_frame: int = 12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Segment the input data into smaller windows based on the given parameters.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Input data with shape (num_samples, num_features).\n",
    "        window_size (int, optional): The length of each window. Defaults to 180.\n",
    "        overlap_ratio (float, optional): The ratio of overlap between consecutive windows. Defaults to 0.75.\n",
    "        by_type (bool, optional): Whether to segment the data by type (assuming the 71st feature is the type). Defaults to True.\n",
    "        min_frame (int, optional): The minimum number of frames required to create a new instance. Defaults to 12.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The segmented data with shape (num_windows, window_size, num_features).\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the input constraints\n",
    "    assert data.shape[0] > 0\n",
    "    assert window_size > 0\n",
    "    assert 0 <= overlap_ratio < 1\n",
    "    assert 0 <= min_frame < window_size\n",
    "\n",
    "    dim = data.shape[1]\n",
    "    instances = []\n",
    "\n",
    "    if not by_type:\n",
    "        instances.append(data)\n",
    "    else:\n",
    "        assert data.shape[1] >= 71\n",
    "\n",
    "        num_data = data.shape[0]\n",
    "        left, right = 0, 1\n",
    "        pre_type = -1\n",
    "        cur_type = data[left, 70]\n",
    "\n",
    "        # Segment the data by exercise type\n",
    "        while right < num_data:\n",
    "            if data[right, 70] == cur_type:\n",
    "                right += 1\n",
    "                continue\n",
    "\n",
    "            if right - left <= min_frame:\n",
    "                left = right\n",
    "                cur_type = data[left, 70]\n",
    "                right += 1\n",
    "                continue\n",
    "\n",
    "            new_instance = np.take(data, range(left, right), axis=0)\n",
    "            if pre_type == new_instance[0, 70]:\n",
    "                instances[-1] = np.vstack([instances[-1], new_instance])\n",
    "            else:\n",
    "                instances.append(new_instance)\n",
    "\n",
    "            left = right\n",
    "            pre_type = cur_type\n",
    "            cur_type = data[left, 70]\n",
    "            right += 1\n",
    "\n",
    "        # Handle the remaining data\n",
    "        new_instance = np.take(data, range(left, right), axis=0)\n",
    "        last = instances[-1]\n",
    "        if last[0, 70] == new_instance[0, 70]:\n",
    "            instances[-1] = np.vstack([last, new_instance])\n",
    "        else:\n",
    "            instances.append(new_instance)\n",
    "\n",
    "    # print(len(instances))\n",
    "\n",
    "    step_size = int(window_size * (1 - overlap_ratio))\n",
    "    windows = []\n",
    "\n",
    "    # Create windows for each instance\n",
    "    for instance in instances:\n",
    "        if instance.shape[0] < window_size:\n",
    "            instance = np.vstack([instance, np.zeros((window_size - instance.shape[0], dim))])\n",
    "            windows.append(instance)\n",
    "            continue\n",
    "\n",
    "        if (instance.shape[0] - window_size) % step_size != 0:\n",
    "            pad_size = step_size - (instance.shape[0] - window_size) % step_size\n",
    "            instance = np.vstack([instance, np.zeros((pad_size, dim))])\n",
    "\n",
    "        for i in range(0, instance.shape[0] - window_size + 1, step_size):\n",
    "            windows.append(np.take(instance, range(i, i + window_size), axis=0))\n",
    "\n",
    "    return np.array(windows)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def fit(estimator: Any, parameters: dict, path: str = train_path, base_parameters: dict = None, seg: bool = False, downsampling: bool = False) -> [Any]:\n",
    "    # The function takes an estimator object, a dictionary of parameters to be used with the estimator, a path to the directory containing the training data,\n",
    "    # a dictionary of parameters to be passed to a base estimator (if any), a flag indicating whether to segment the data or not, and a flag indicating whether to use downsampling.\n",
    "\n",
    "    # Create an empty list to store the models\n",
    "    models = []\n",
    "\n",
    "    # Get the class of the estimator object\n",
    "    estimator_class = type(estimator)\n",
    "\n",
    "    # Iterate over all files in the directory pointed by 'path'\n",
    "    for i, file in tqdm(enumerate(os.listdir(path))):\n",
    "        # Load data from the file in a Matlab format using SciPy's 'loadmat' function\n",
    "        mat = scipy.io.loadmat(os.path.join(path, file))\n",
    "\n",
    "        # Extract the data from the dictionary using the 'data' key\n",
    "        data = mat['data']\n",
    "\n",
    "        # If downsampling flag is True and all labels are the same, then skip this file and go to the next iteration\n",
    "        if downsampling and np.unique(data[:, 72:73]).size == 1:\n",
    "            print(\"downsampling!!!\")\n",
    "            continue\n",
    "\n",
    "        # If a base estimator is specified, create an instance of it with the given base parameters and replace the 'base_estimator' key in the parameters dictionary with this instance\n",
    "        if 'base_estimator' in parameters:\n",
    "            base_class = type(parameters['base_estimator'])\n",
    "            parameters['base_estimator'] = base_class(**base_parameters) if base_parameters else base_class()\n",
    "\n",
    "        # If the 'seg' flag is True, segment the data and reshape X_train\n",
    "        if seg:\n",
    "            data = segment_data(data)\n",
    "            X_train = data[:, :, 0:70]\n",
    "            X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "            y_train = data[:, :, 72:73]\n",
    "            # Compute the mode along the second axis of the y_train array to obtain the most frequent class in each segment\n",
    "            y_train = np.apply_along_axis(lambda x: mode(x)[0], 1, y_train[:, :, 0]).flatten()\n",
    "        # If the 'seg' flag is False, simply remove the columns containing the labels from the data to obtain X_train and keep only the column with labels to obtain y_train\n",
    "        else:\n",
    "            X_train = np.delete(data, range(70, 78), axis=1)\n",
    "            y_train = data[:, 72]\n",
    "\n",
    "        # Create a new instance of the estimator class with the specified parameters and fit the model to the training data\n",
    "        model = estimator_class(**parameters)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Append the tuple containing the file name and the trained model object to the models list\n",
    "        models.append((file, model))\n",
    "\n",
    "    print(\"Downsampling:\", downsampling)\n",
    "    return models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def validate(models: list, path: str = validation_path, seg: bool = False) -> None:\n",
    "    # The function takes a list of models, a path to the directory containing the validation data, and a flag indicating whether to segment the data or not\n",
    "\n",
    "    # Initialize variables to store the number of true protective samples, total protective samples, true non-protective samples, and total non-protective samples\n",
    "    total_true_protective = 0\n",
    "    total_protective = 0\n",
    "    total_true_non_protective = 0\n",
    "    total_non_protective = 0\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        mat = scipy.io.loadmat(os.path.join(path, file))\n",
    "        data = mat['data']\n",
    "\n",
    "        # If the 'seg' flag is True, segment the data and reshape X_validation\n",
    "        if seg:\n",
    "            data = segment_data(data)\n",
    "            X_validation = data[:, :, 0:70]\n",
    "            X_validation = X_validation.reshape(X_validation.shape[0], -1)\n",
    "            y_validation = data[:, :, 72:73]\n",
    "            # Compute the mode along the second axis of the y_validation array to obtain the most frequent class in each segment\n",
    "            y_validation = np.apply_along_axis(lambda x: mode(x)[0], 1, y_validation[:, :, 0]).flatten()\n",
    "            assert X_validation.shape[0] == y_validation.shape[0]\n",
    "        # If the 'seg' flag is False, simply remove the columns containing the labels from the data to obtain X_validation and keep only the column with labels to obtain y_validation\n",
    "        else:\n",
    "            X_validation = np.delete(data, range(70, 78), axis=1)\n",
    "            y_validation = data[:, 72]\n",
    "\n",
    "        # Initialize an empty list to store the predictions from all models\n",
    "        predictions = []\n",
    "        for model in models:\n",
    "            predictions.append(model[1].predict(X_validation))\n",
    "\n",
    "        # Compute the mode of the predictions along the first axis to obtain the final prediction for each sample in X_validation\n",
    "        y_pred, _ = mode(predictions, axis=0)\n",
    "        y_pred = y_pred.flatten()\n",
    "\n",
    "        protective = np.where(y_validation == 1)\n",
    "        non_protective = np.where(y_validation == 0)\n",
    "\n",
    "        # Compute the number of true protective samples, total protective samples, true non-protective samples, and total non-protective samples\n",
    "        total_true_protective += np.sum(y_validation[protective] == y_pred[protective])\n",
    "        total_protective += len(y_validation[protective])\n",
    "\n",
    "        total_true_non_protective += np.sum(y_validation[non_protective] == y_pred[non_protective])\n",
    "        total_non_protective += len(y_validation[non_protective])\n",
    "\n",
    "    print(\"Files are segmented:\", seg)\n",
    "    print(\"Accuracy(protective):\", total_true_protective / total_protective, total_true_protective, total_protective)\n",
    "    print(\"Accuracy(non-protective):\", total_true_non_protective / total_non_protective, total_true_non_protective, total_non_protective)\n",
    "    TN, FN = total_true_non_protective, total_non_protective - total_true_non_protective\n",
    "    FP, TP = total_protective - total_true_protective, total_true_protective\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = 0.0 if TP == 0.0 else TP / (TP + FN)\n",
    "    f1_score = 0.0 if TP == 0.0 else 2 * (precision * recall) / (precision + recall)\n",
    "    print(\"F1-score:\", f1_score)\n",
    "    print(\"Overall accuracy:\", (total_true_protective + total_true_non_protective) / (total_protective + total_non_protective))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [03:27,  9.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 15.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are segmented: False\n",
      "Accuracy(protective): 0.0 0 10721\n",
      "Accuracy(non-protective): 0.9999932302984761 147716 147717\n",
      "F1-score: 0.0\n",
      "Overall accuracy: 0.9323268407831454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [03:26,  8.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: True\n",
      "Files are segmented: False\n",
      "Accuracy(protective): 0.34381121164070516 3686 10721\n",
      "Accuracy(non-protective): 0.5322271641043347 78619 147717\n",
      "F1-score: 0.0882821387940842\n",
      "Overall accuracy: 0.5194776505636274\n"
     ]
    }
   ],
   "source": [
    "rfcs = fit(RandomForestClassifier(), {'random_state': 42})\n",
    "validate(rfcs)\n",
    "rfcs = fit(RandomForestClassifier(), {'random_state': 42}, downsampling=True)\n",
    "validate(rfcs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [01:19,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 34.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are segmented: False\n",
      "Accuracy(protective): 0.0 0 10721\n",
      "Accuracy(non-protective): 1.0 147717 147717\n",
      "F1-score: 0.0\n",
      "Overall accuracy: 0.932333152400308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [01:18,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: True\n",
      "Files are segmented: False\n",
      "Accuracy(protective): 0.06435966794142338 690 10721\n",
      "Accuracy(non-protective): 0.8747672915101173 129218 147717\n",
      "F1-score: 0.04613841524573721\n",
      "Overall accuracy: 0.819929562352466\n"
     ]
    }
   ],
   "source": [
    "abcs = fit(AdaBoostClassifier(), {'random_state': 42})\n",
    "validate(abcs)\n",
    "abcs = fit(AdaBoostClassifier(), {'random_state': 42}, downsampling=True)\n",
    "validate(abcs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:14,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 35.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are segmented: False\n",
      "Accuracy(protective): 0.0427198955321332 458 10721\n",
      "Accuracy(non-protective): 0.9379286067277294 138548 147717\n",
      "F1-score: 0.04501670925889523\n",
      "Overall accuracy: 0.8773526552973403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:13,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: True\n",
      "Files are segmented: False\n",
      "Accuracy(protective): 0.6675683238503871 7157 10721\n",
      "Accuracy(non-protective): 0.18035838799867313 26642 147717\n",
      "F1-score: 0.10301324908422273\n",
      "Overall accuracy: 0.21332634847700677\n"
     ]
    }
   ],
   "source": [
    "abcs_dtc = fit(AdaBoostClassifier(), {'base_estimator': DecisionTreeClassifier(), 'random_state': 42})\n",
    "validate(abcs_dtc)\n",
    "abcs_dtc = fit(AdaBoostClassifier(), {'base_estimator': DecisionTreeClassifier(), 'random_state': 42}, downsampling=True)\n",
    "validate(abcs_dtc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:13,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 35.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are segmented: True\n",
      "Accuracy(protective): 0.0 0 171\n",
      "Accuracy(non-protective): 0.9985174203113417 2694 2698\n",
      "F1-score: 0.0\n",
      "Overall accuracy: 0.9390031369815267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:12,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: True\n",
      "Files are segmented: True\n",
      "Accuracy(protective): 0.39766081871345027 68 171\n",
      "Accuracy(non-protective): 0.6942179392142328 1873 2698\n",
      "F1-score: 0.12781954887218044\n",
      "Overall accuracy: 0.67654234925061\n"
     ]
    }
   ],
   "source": [
    "rfcs_seg = fit(RandomForestClassifier(), {'random_state': 42}, seg=True)\n",
    "validate(rfcs_seg, seg=True)\n",
    "rfcs_seg = fit(RandomForestClassifier(), {'random_state': 42}, seg=True, downsampling=True)\n",
    "validate(rfcs_seg, seg=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [03:13,  8.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 35.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are segmented: True\n",
      "Accuracy(protective): 0.0 0 171\n",
      "Accuracy(non-protective): 1.0 2698 2698\n",
      "F1-score: 0.0\n",
      "Overall accuracy: 0.9403973509933775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [03:14,  8.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: True\n",
      "Files are segmented: True\n",
      "Accuracy(protective): 0.2982456140350877 51 171\n",
      "Accuracy(non-protective): 0.9521868050407709 2569 2698\n",
      "F1-score: 0.2905982905982906\n",
      "Overall accuracy: 0.9132101777622865\n"
     ]
    }
   ],
   "source": [
    "abcs_seg = fit(AdaBoostClassifier(), {'random_state': 42}, seg=True)\n",
    "validate(abcs_seg, seg=True)\n",
    "abcs_seg = fit(AdaBoostClassifier(), {'random_state': 42}, seg=True, downsampling=True)\n",
    "validate(abcs_seg, seg=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:20,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 35.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are segmented: True\n",
      "Accuracy(protective): 0.0 0 171\n",
      "Accuracy(non-protective): 1.0 2698 2698\n",
      "F1-score: 0.0\n",
      "Overall accuracy: 0.9403973509933775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:19,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: True\n",
      "Files are segmented: True\n",
      "Accuracy(protective): 0.09941520467836257 17 171\n",
      "Accuracy(non-protective): 0.9844329132690882 2656 2698\n",
      "F1-score: 0.14782608695652175\n",
      "Overall accuracy: 0.9316835134193099\n"
     ]
    }
   ],
   "source": [
    "abcs_dtc_seg = fit(AdaBoostClassifier(), {'base_estimator': DecisionTreeClassifier(), 'random_state': 42}, seg=True)\n",
    "validate(abcs_dtc_seg, seg=True)\n",
    "abcs_dtc_seg = fit(AdaBoostClassifier(), {'base_estimator': DecisionTreeClassifier(), 'random_state': 42}, seg=True, downsampling=True)\n",
    "validate(abcs_dtc_seg, seg=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
