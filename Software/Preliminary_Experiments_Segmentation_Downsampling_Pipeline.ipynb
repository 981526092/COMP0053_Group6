{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import mode\n",
    "from tqdm import tqdm\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_path = \"CoordinateData\\\\train\"\n",
    "validation_path = \"CoordinateData\\\\validation\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def segment_data(data: np.ndarray, window_size: int = 180, overlap_ratio: float = 0.75, by_type: bool = True, min_frame: int = 12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Segment the input data into smaller windows based on the given parameters.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Input data with shape (num_samples, num_features).\n",
    "        window_size (int, optional): The length of each window. Defaults to 180.\n",
    "        overlap_ratio (float, optional): The ratio of overlap between consecutive windows. Defaults to 0.75.\n",
    "        by_type (bool, optional): Whether to segment the data by type (assuming the 71st feature is the type). Defaults to True.\n",
    "        min_frame (int, optional): The minimum number of frames required to create a new instance. Defaults to 12.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The segmented data with shape (num_windows, window_size, num_features).\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the input constraints\n",
    "    assert data.shape[0] > 0\n",
    "    assert window_size > 0\n",
    "    assert 0 <= overlap_ratio < 1\n",
    "    assert 0 <= min_frame < window_size\n",
    "\n",
    "    dim = data.shape[1]\n",
    "    instances = []\n",
    "\n",
    "    if not by_type:\n",
    "        instances.append(data)\n",
    "    else:\n",
    "        assert data.shape[1] >= 71\n",
    "\n",
    "        num_data = data.shape[0]\n",
    "        left, right = 0, 1\n",
    "        pre_type = -1\n",
    "        cur_type = data[left, 70]\n",
    "\n",
    "        # Segment the data by exercise type\n",
    "        while right < num_data:\n",
    "            if data[right, 70] == cur_type:\n",
    "                right += 1\n",
    "                continue\n",
    "\n",
    "            if right - left <= min_frame:\n",
    "                left = right\n",
    "                cur_type = data[left, 70]\n",
    "                right += 1\n",
    "                continue\n",
    "\n",
    "            new_instance = np.take(data, range(left, right), axis=0)\n",
    "            if pre_type == new_instance[0, 70]:\n",
    "                instances[-1] = np.vstack([instances[-1], new_instance])\n",
    "            else:\n",
    "                instances.append(new_instance)\n",
    "\n",
    "            left = right\n",
    "            pre_type = cur_type\n",
    "            cur_type = data[left, 70]\n",
    "            right += 1\n",
    "\n",
    "        # Handle the remaining data\n",
    "        new_instance = np.take(data, range(left, right), axis=0)\n",
    "        last = instances[-1]\n",
    "        if last[0, 70] == new_instance[0, 70]:\n",
    "            instances[-1] = np.vstack([last, new_instance])\n",
    "        else:\n",
    "            instances.append(new_instance)\n",
    "\n",
    "    # print(len(instances))\n",
    "\n",
    "    step_size = int(window_size * (1 - overlap_ratio))\n",
    "    windows = []\n",
    "\n",
    "    # Create windows for each instance\n",
    "    for instance in instances:\n",
    "        if instance.shape[0] < window_size:\n",
    "            instance = np.vstack([instance, np.zeros((window_size - instance.shape[0], dim))])\n",
    "            windows.append(instance)\n",
    "            continue\n",
    "\n",
    "        if (instance.shape[0] - window_size) % step_size != 0:\n",
    "            pad_size = step_size - (instance.shape[0] - window_size) % step_size\n",
    "            instance = np.vstack([instance, np.zeros((pad_size, dim))])\n",
    "\n",
    "        for i in range(0, instance.shape[0] - window_size + 1, step_size):\n",
    "            windows.append(np.take(instance, range(i, i + window_size), axis=0))\n",
    "\n",
    "    return np.array(windows)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def ensemble_predict(y_predicts: np.ndarray, confusion_matrices: np.ndarray, ensemble_method: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform ensemble prediction using different methods based on multiple models' predictions and their confusion matrices.\n",
    "\n",
    "    Parameters:\n",
    "    y_predicts (np.ndarray): A 3D array of shape (num_modalities, num_windows, 2) containing prediction probabilities for each modality.\n",
    "    confusion_matrices (np.ndarray): A 3D array of shape (num_modalities, 2, 2) containing confusion matrices for each modality.\n",
    "    ensemble_method (int): An integer (0-7) indicating the ensemble method to be used. Default is 0 (Simple Average).\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 1D array of shape (num_windows,) containing the ensemble predictions for each window.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check input assertions\n",
    "    assert y_predicts.shape[0] > 1\n",
    "    assert y_predicts.shape[2] == 2\n",
    "    assert y_predicts.shape[0] == confusion_matrices.shape[0]\n",
    "    assert 0 <= ensemble_method < 8\n",
    "\n",
    "    num_modalities = y_predicts.shape[0]\n",
    "    num_windows = y_predicts.shape[1]\n",
    "    print(f'{num_windows} data are predicted using combined results of {num_modalities} modalities')\n",
    "\n",
    "    # Perform ensemble prediction based on the selected method\n",
    "    if ensemble_method == 0:                                            # Simple Average\n",
    "        final_predict = np.argmax(np.mean(y_predicts, axis=0), axis=1)\n",
    "    elif ensemble_method in [1, 2, 3, 4]:                               # Weighted Average (using different metrics)\n",
    "        accuracies = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1_scores = []\n",
    "\n",
    "        # Calculate metrics for each confusion matrix\n",
    "        for cm in confusion_matrices:\n",
    "            TN, FN, FP, TP = cm.ravel()\n",
    "\n",
    "            accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "            precision = TP / (TP + FP)\n",
    "            recall = 0.0 if TP == 0.0 else TP / (TP + FN)\n",
    "            f1_score = 0.0 if TP == 0.0 else 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "            accuracies.append(accuracy)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1_score)\n",
    "\n",
    "        # Normalize the metrics\n",
    "        accuracies = np.array(accuracies) / np.sum(accuracies)\n",
    "        precisions = np.array(precisions) if np.sum(precisions) == 0 else np.array(precisions) / np.sum(precisions)\n",
    "        recalls = np.array(recalls) if np.sum(recalls) == 0 else np.array(recalls) / np.sum(recalls)\n",
    "        f1_scores = np.array(f1_scores) if np.sum(f1_scores) == 0 else np.array(f1_scores) / np.sum(f1_scores)\n",
    "\n",
    "        metrics = {\n",
    "                1: accuracies,\n",
    "                2: precisions,\n",
    "                3: recalls,\n",
    "                4: f1_scores\n",
    "        }\n",
    "        # Choose the appropriate metric for weighted averaging\n",
    "        metric = metrics[ensemble_method]\n",
    "\n",
    "        # Perform weighted averaging\n",
    "        y_predicts = metric[:, np.newaxis, np.newaxis] * y_predicts\n",
    "        final_predict = np.argmax(np.sum(y_predicts, axis=0), axis=1)\n",
    "    elif ensemble_method == 5:                                          # Maximum Rule\n",
    "        final_predict = np.argmax(np.max(y_predicts, axis=0), axis=1)\n",
    "    elif ensemble_method == 6:                                          # Product Rule\n",
    "        final_predict = np.argmax(np.prod(y_predicts, axis=0), axis=1)\n",
    "    else:                                                               # Voting\n",
    "        final_predict, _ = mode(np.argmax(y_predicts, axis=2), axis=0)\n",
    "        final_predict = final_predict.flatten()\n",
    "\n",
    "    assert final_predict.shape[0] == y_predicts.shape[1]\n",
    "    return final_predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def fit(estimator: Any, parameters: dict, path: str = train_path, base_parameters: dict = None, seg: bool = False, downsampling: bool = False) -> [Any]:\n",
    "    models = []\n",
    "    estimator_class = type(estimator)\n",
    "\n",
    "    for i, file in tqdm(enumerate(os.listdir(path))):\n",
    "        mat = scipy.io.loadmat(os.path.join(path, file))\n",
    "        data = mat['data']\n",
    "        if downsampling and np.unique(data[:, 72:73]).size == 1:\n",
    "            print(\"downsampling!!!\")\n",
    "            continue\n",
    "\n",
    "        if 'base_estimator' in parameters:\n",
    "            base_class = type(parameters['base_estimator'])\n",
    "            parameters['base_estimator'] = base_class(**base_parameters) if base_parameters else base_class()\n",
    "\n",
    "        if seg:\n",
    "            data = segment_data(data)\n",
    "            X_train = data[:, :, 0:70]\n",
    "            X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "            y_train = data[:, :, 72:73]\n",
    "            y_train = np.apply_along_axis(lambda x: mode(x)[0], 1, y_train[:, :, 0]).flatten()\n",
    "        else:\n",
    "            X_train = np.delete(data, range(70, 78), axis=1)\n",
    "            y_train = data[:, 72]\n",
    "\n",
    "        model = estimator_class(**parameters)\n",
    "        model.fit(X_train, y_train)\n",
    "        models.append((file, model))\n",
    "\n",
    "    print(\"Downsampling:\", downsampling)\n",
    "    return models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def validate(models: list, path: str = validation_path, seg: bool = False) -> None:\n",
    "    total_true_protective = 0\n",
    "    total_protective = 0\n",
    "    total_true_non_protective = 0\n",
    "    total_non_protective = 0\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        mat = scipy.io.loadmat(os.path.join(path, file))\n",
    "        data = mat['data']\n",
    "        if seg:\n",
    "            data = segment_data(data)\n",
    "            X_validation = data[:, :, 0:70]\n",
    "            X_validation = X_validation.reshape(X_validation.shape[0], -1)\n",
    "            y_validation = data[:, :, 72:73]\n",
    "            y_validation = np.apply_along_axis(lambda x: mode(x)[0], 1, y_validation[:, :, 0]).flatten()\n",
    "            assert X_validation.shape[0] == y_validation.shape[0]\n",
    "        else:\n",
    "            X_validation = np.delete(data, range(70, 78), axis=1)\n",
    "            y_validation = data[:, 72]\n",
    "\n",
    "        predictions = []\n",
    "        for model in models:\n",
    "            predictions.append(model[1].predict(X_validation))\n",
    "\n",
    "        y_pred, _ = mode(predictions, axis=0)\n",
    "        y_pred = y_pred.flatten()\n",
    "\n",
    "        protective = np.where(y_validation == 1)\n",
    "        non_protective = np.where(y_validation == 0)\n",
    "\n",
    "        total_true_protective += np.sum(y_validation[protective] == y_pred[protective])\n",
    "        total_protective += len(y_validation[protective])\n",
    "\n",
    "        total_true_non_protective += np.sum(y_validation[non_protective] == y_pred[non_protective])\n",
    "        total_non_protective += len(y_validation[non_protective])\n",
    "\n",
    "    print(\"Files are segmented:\", seg)\n",
    "    print(\"Accuracy(protective):\", total_true_protective / total_protective, total_true_protective, total_protective)\n",
    "    print(\"Accuracy(non-protective):\", total_true_non_protective / total_non_protective, total_true_non_protective, total_non_protective)\n",
    "    TN, FN = total_true_non_protective, total_non_protective - total_true_non_protective\n",
    "    FP, TP = total_protective - total_true_protective, total_true_protective\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = 0.0 if TP == 0.0 else TP / (TP + FN)\n",
    "    f1_score = 0.0 if TP == 0.0 else 2 * (precision * recall) / (precision + recall)\n",
    "    print(\"F1-score:\", f1_score)\n",
    "    print(\"Overall accuracy:\", (total_true_protective + total_true_non_protective) / (total_protective + total_non_protective))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [03:27,  9.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 15.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are segmented: False\n",
      "Accuracy(protective): 0.0 0 10721\n",
      "Accuracy(non-protective): 0.9999932302984761 147716 147717\n",
      "F1-score: 0.0\n",
      "Overall accuracy: 0.9323268407831454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [03:26,  8.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: True\n",
      "Files are segmented: False\n",
      "Accuracy(protective): 0.34381121164070516 3686 10721\n",
      "Accuracy(non-protective): 0.5322271641043347 78619 147717\n",
      "F1-score: 0.0882821387940842\n",
      "Overall accuracy: 0.5194776505636274\n"
     ]
    }
   ],
   "source": [
    "rfcs = fit(RandomForestClassifier(), {'random_state': 42})\n",
    "validate(rfcs)\n",
    "rfcs = fit(RandomForestClassifier(), {'random_state': 42}, downsampling=True)\n",
    "validate(rfcs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [01:19,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 34.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are segmented: False\n",
      "Accuracy(protective): 0.0 0 10721\n",
      "Accuracy(non-protective): 1.0 147717 147717\n",
      "F1-score: 0.0\n",
      "Overall accuracy: 0.932333152400308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [01:18,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: True\n",
      "Files are segmented: False\n",
      "Accuracy(protective): 0.06435966794142338 690 10721\n",
      "Accuracy(non-protective): 0.8747672915101173 129218 147717\n",
      "F1-score: 0.04613841524573721\n",
      "Overall accuracy: 0.819929562352466\n"
     ]
    }
   ],
   "source": [
    "abcs = fit(AdaBoostClassifier(), {'random_state': 42})\n",
    "validate(abcs)\n",
    "abcs = fit(AdaBoostClassifier(), {'random_state': 42}, downsampling=True)\n",
    "validate(abcs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:14,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 35.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are segmented: False\n",
      "Accuracy(protective): 0.0427198955321332 458 10721\n",
      "Accuracy(non-protective): 0.9379286067277294 138548 147717\n",
      "F1-score: 0.04501670925889523\n",
      "Overall accuracy: 0.8773526552973403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:13,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: True\n",
      "Files are segmented: False\n",
      "Accuracy(protective): 0.6675683238503871 7157 10721\n",
      "Accuracy(non-protective): 0.18035838799867313 26642 147717\n",
      "F1-score: 0.10301324908422273\n",
      "Overall accuracy: 0.21332634847700677\n"
     ]
    }
   ],
   "source": [
    "abcs_dtc = fit(AdaBoostClassifier(), {'base_estimator': DecisionTreeClassifier(), 'random_state': 42})\n",
    "validate(abcs_dtc)\n",
    "abcs_dtc = fit(AdaBoostClassifier(), {'base_estimator': DecisionTreeClassifier(), 'random_state': 42}, downsampling=True)\n",
    "validate(abcs_dtc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:13,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 35.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are segmented: True\n",
      "Accuracy(protective): 0.0 0 171\n",
      "Accuracy(non-protective): 0.9985174203113417 2694 2698\n",
      "F1-score: 0.0\n",
      "Overall accuracy: 0.9390031369815267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:12,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: True\n",
      "Files are segmented: True\n",
      "Accuracy(protective): 0.39766081871345027 68 171\n",
      "Accuracy(non-protective): 0.6942179392142328 1873 2698\n",
      "F1-score: 0.12781954887218044\n",
      "Overall accuracy: 0.67654234925061\n"
     ]
    }
   ],
   "source": [
    "rfcs_seg = fit(RandomForestClassifier(), {'random_state': 42}, seg=True)\n",
    "validate(rfcs_seg, seg=True)\n",
    "rfcs_seg = fit(RandomForestClassifier(), {'random_state': 42}, seg=True, downsampling=True)\n",
    "validate(rfcs_seg, seg=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [03:13,  8.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 35.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are segmented: True\n",
      "Accuracy(protective): 0.0 0 171\n",
      "Accuracy(non-protective): 1.0 2698 2698\n",
      "F1-score: 0.0\n",
      "Overall accuracy: 0.9403973509933775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [03:14,  8.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: True\n",
      "Files are segmented: True\n",
      "Accuracy(protective): 0.2982456140350877 51 171\n",
      "Accuracy(non-protective): 0.9521868050407709 2569 2698\n",
      "F1-score: 0.2905982905982906\n",
      "Overall accuracy: 0.9132101777622865\n"
     ]
    }
   ],
   "source": [
    "abcs_seg = fit(AdaBoostClassifier(), {'random_state': 42}, seg=True)\n",
    "validate(abcs_seg, seg=True)\n",
    "abcs_seg = fit(AdaBoostClassifier(), {'random_state': 42}, seg=True, downsampling=True)\n",
    "validate(abcs_seg, seg=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:20,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 35.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are segmented: True\n",
      "Accuracy(protective): 0.0 0 171\n",
      "Accuracy(non-protective): 1.0 2698 2698\n",
      "F1-score: 0.0\n",
      "Overall accuracy: 0.9403973509933775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:19,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling: True\n",
      "Files are segmented: True\n",
      "Accuracy(protective): 0.09941520467836257 17 171\n",
      "Accuracy(non-protective): 0.9844329132690882 2656 2698\n",
      "F1-score: 0.14782608695652175\n",
      "Overall accuracy: 0.9316835134193099\n"
     ]
    }
   ],
   "source": [
    "abcs_dtc_seg = fit(AdaBoostClassifier(), {'base_estimator': DecisionTreeClassifier(), 'random_state': 42}, seg=True)\n",
    "validate(abcs_dtc_seg, seg=True)\n",
    "abcs_dtc_seg = fit(AdaBoostClassifier(), {'base_estimator': DecisionTreeClassifier(), 'random_state': 42}, seg=True, downsampling=True)\n",
    "validate(abcs_dtc_seg, seg=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "def load_data(path: str = train_path, downsampling: bool = True):\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "\n",
    "    selected_data_list = []\n",
    "    for file in os.listdir(path):\n",
    "        mat = scipy.io.loadmat(os.path.join(path, file))\n",
    "        if downsampling and np.unique(mat['data'][:, 72:73]).size == 1:\n",
    "            continue\n",
    "        else:\n",
    "            selected_data_list.append(mat['data'])\n",
    "\n",
    "\n",
    "    for i in range(len(selected_data_list)):\n",
    "        processed_data = segment_data(selected_data_list[i], min_frame=0)\n",
    "\n",
    "        X_segmented = processed_data[:,:,0:70]\n",
    "        y_segmented = processed_data[:,:,72:73]\n",
    "\n",
    "        y_segmented = np.apply_along_axis(lambda x: mode(x)[0], 1, y_segmented[:, :, 0])\n",
    "\n",
    "        X_train_list.append(X_segmented)\n",
    "        y_train_list.append(y_segmented)\n",
    "\n",
    "    X_train_concat = np.concatenate(X_train_list, axis=0)\n",
    "    y_train_concat = np.concatenate(y_train_list, axis=0)\n",
    "\n",
    "    return X_train_concat, y_train_concat.flatten()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5014, 180, 70) (5014,)\n"
     ]
    }
   ],
   "source": [
    "X_t, y_t = load_data()\n",
    "print(X_t.shape, y_t.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4011, 180, 70)\n",
      "(4011,)\n",
      "(1003, 180, 70)\n",
      "(1003,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(X_t, y_t, test_size=0.2, random_state=42)\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4011, 11880) (4011, 720) (1003, 11880) (1003, 720)\n"
     ]
    }
   ],
   "source": [
    "X_t_rf = np.take(train_data, range(0, 66), axis=2).reshape(train_data.shape[0], -1)\n",
    "# std_scaler = StandardScaler()\n",
    "# X_t_svm = std_scaler.fit_transform(np.take(train_data, range(66, 70), axis=2).reshape(train_data.shape[0], -1))\n",
    "X_t_svm = np.take(train_data, range(66, 70), axis=2).reshape(train_data.shape[0], -1)\n",
    "\n",
    "X_v_rf = np.take(test_data, range(0, 66), axis=2).reshape(test_data.shape[0], -1)\n",
    "# X_v_svm = std_scaler.transform(np.take(test_data, range(66, 70), axis=2).reshape(test_data.shape[0], -1))\n",
    "X_v_svm = np.take(test_data, range(66, 70), axis=2).reshape(test_data.shape[0], -1)\n",
    "\n",
    "print(X_t_rf.shape, X_t_svm.shape, X_v_rf.shape, X_v_svm.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(random_state=42)"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_t_rf, train_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "rf_confusion_matrix = confusion_matrix(test_labels, rf.predict(X_v_rf))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def hard_late_fusion(SVC_parameters: dict) -> None:\n",
    "    svm = SVC(**SVC_parameters)\n",
    "    svm.fit(X_t_svm, train_labels)\n",
    "\n",
    "    svm_confusion_matrix = confusion_matrix(test_labels, svm.predict(X_v_svm))\n",
    "\n",
    "    print(\"RF confusion matrix\")\n",
    "    print(rf_confusion_matrix)\n",
    "    print(\"svm confusion matrix\")\n",
    "    print(svm_confusion_matrix)\n",
    "\n",
    "    confusion_matrices = np.array([rf_confusion_matrix, svm_confusion_matrix])\n",
    "    total_true_protective = 0\n",
    "    total_protective = 0\n",
    "    total_true_non_protective = 0\n",
    "    total_non_protective = 0\n",
    "\n",
    "    for file in os.listdir(validation_path):\n",
    "        mat = scipy.io.loadmat(os.path.join(validation_path, file))\n",
    "        data = segment_data(mat['data'], min_frame=0)\n",
    "        X_validation_rf = np.take(data, range(0, 66), axis=2).reshape(data.shape[0], -1)\n",
    "        X_validation_svm = np.take(data, range(66, 70), axis=2).reshape(data.shape[0], -1)\n",
    "        y_validation = data[:, :, 72:73]\n",
    "        y_validation = np.apply_along_axis(lambda x: mode(x)[0], 1, y_validation).flatten()\n",
    "\n",
    "        rf_pred = to_categorical(rf.predict(X_validation_rf), 2)\n",
    "        # svm_pred = to_categorical(svm.predict(std_scaler.transform(X_validation_svm)), 2)\n",
    "        svm_pred = to_categorical(svm.predict(X_validation_svm), 2)\n",
    "        print(rf_pred.shape, svm_pred.shape, y_validation.shape)\n",
    "\n",
    "        protective = np.where(y_validation == 1)[0]\n",
    "        non_protective = np.where(y_validation == 0)[0]\n",
    "\n",
    "        y_pred = ensemble_predict(np.array([rf_pred, svm_pred]), confusion_matrices, ensemble_method=6)\n",
    "\n",
    "        total_true_protective += np.sum(y_pred[protective] == y_validation[protective])\n",
    "        total_protective += len(y_validation[protective])\n",
    "\n",
    "        total_true_non_protective += np.sum(y_pred[non_protective] == y_validation[non_protective])\n",
    "        total_non_protective += len(y_validation[non_protective])\n",
    "\n",
    "    print(\"Accuracy(protective):\", total_true_protective / total_protective, total_true_protective, total_protective)\n",
    "    print(\"Accuracy(non-protective):\", total_true_non_protective / total_non_protective, total_true_non_protective, total_non_protective)\n",
    "    TN, FN = total_true_non_protective, total_non_protective - total_true_non_protective\n",
    "    FP, TP = total_protective - total_true_protective, total_true_protective\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = 0.0 if TP == 0.0 else TP / (TP + FN)\n",
    "    f1_score = 0.0 if TP == 0.0 else 2 * (precision * recall) / (precision + recall)\n",
    "    print(\"F1-score:\", f1_score)\n",
    "    print(\"Overall accuracy:\", (total_true_protective + total_true_non_protective) / (total_protective + total_non_protective))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF confusion matrix\n",
      "[[785  22]\n",
      " [ 26 170]]\n",
      "svm confusion matrix\n",
      "[[664 143]\n",
      " [133  63]]\n",
      "(197, 2) (197, 2) (197,)\n",
      "197 data are predicted using combined results of 2 modalities\n",
      "(169, 2) (169, 2) (169,)\n",
      "169 data are predicted using combined results of 2 modalities\n",
      "(218, 2) (218, 2) (218,)\n",
      "218 data are predicted using combined results of 2 modalities\n",
      "(181, 2) (181, 2) (181,)\n",
      "181 data are predicted using combined results of 2 modalities\n",
      "(276, 2) (276, 2) (276,)\n",
      "276 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(185, 2) (185, 2) (185,)\n",
      "185 data are predicted using combined results of 2 modalities\n",
      "(230, 2) (230, 2) (230,)\n",
      "230 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(267, 2) (267, 2) (267,)\n",
      "267 data are predicted using combined results of 2 modalities\n",
      "(254, 2) (254, 2) (254,)\n",
      "254 data are predicted using combined results of 2 modalities\n",
      "(345, 2) (345, 2) (345,)\n",
      "345 data are predicted using combined results of 2 modalities\n",
      "Accuracy(protective): 0.25903614457831325 43 166\n",
      "Accuracy(non-protective): 0.9523281596452328 2577 2706\n",
      "F1-score: 0.25443786982248523\n",
      "Overall accuracy: 0.9122562674094707\n"
     ]
    }
   ],
   "source": [
    "hard_late_fusion({'kernel': 'linear', 'random_state': 42, 'class_weight': 'balanced'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF confusion matrix\n",
      "[[785  22]\n",
      " [ 26 170]]\n",
      "svm confusion matrix\n",
      "[[762  45]\n",
      " [151  45]]\n",
      "(197, 2) (197, 2) (197,)\n",
      "197 data are predicted using combined results of 2 modalities\n",
      "(169, 2) (169, 2) (169,)\n",
      "169 data are predicted using combined results of 2 modalities\n",
      "(218, 2) (218, 2) (218,)\n",
      "218 data are predicted using combined results of 2 modalities\n",
      "(181, 2) (181, 2) (181,)\n",
      "181 data are predicted using combined results of 2 modalities\n",
      "(276, 2) (276, 2) (276,)\n",
      "276 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(185, 2) (185, 2) (185,)\n",
      "185 data are predicted using combined results of 2 modalities\n",
      "(230, 2) (230, 2) (230,)\n",
      "230 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(267, 2) (267, 2) (267,)\n",
      "267 data are predicted using combined results of 2 modalities\n",
      "(254, 2) (254, 2) (254,)\n",
      "254 data are predicted using combined results of 2 modalities\n",
      "(345, 2) (345, 2) (345,)\n",
      "345 data are predicted using combined results of 2 modalities\n",
      "Accuracy(protective): 0.13253012048192772 22 166\n",
      "Accuracy(non-protective): 0.9852180339985218 2666 2706\n",
      "F1-score: 0.1929824561403509\n",
      "Overall accuracy: 0.935933147632312\n"
     ]
    }
   ],
   "source": [
    "hard_late_fusion({'kernel': 'poly', 'degree': 2, 'random_state': 42, 'class_weight': 'balanced'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF confusion matrix\n",
      "[[785  22]\n",
      " [ 26 170]]\n",
      "svm confusion matrix\n",
      "[[767  40]\n",
      " [152  44]]\n",
      "(197, 2) (197, 2) (197,)\n",
      "197 data are predicted using combined results of 2 modalities\n",
      "(169, 2) (169, 2) (169,)\n",
      "169 data are predicted using combined results of 2 modalities\n",
      "(218, 2) (218, 2) (218,)\n",
      "218 data are predicted using combined results of 2 modalities\n",
      "(181, 2) (181, 2) (181,)\n",
      "181 data are predicted using combined results of 2 modalities\n",
      "(276, 2) (276, 2) (276,)\n",
      "276 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(185, 2) (185, 2) (185,)\n",
      "185 data are predicted using combined results of 2 modalities\n",
      "(230, 2) (230, 2) (230,)\n",
      "230 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(267, 2) (267, 2) (267,)\n",
      "267 data are predicted using combined results of 2 modalities\n",
      "(254, 2) (254, 2) (254,)\n",
      "254 data are predicted using combined results of 2 modalities\n",
      "(345, 2) (345, 2) (345,)\n",
      "345 data are predicted using combined results of 2 modalities\n",
      "Accuracy(protective): 0.12650602409638553 21 166\n",
      "Accuracy(non-protective): 0.984109386548411 2663 2706\n",
      "F1-score: 0.18260869565217389\n",
      "Overall accuracy: 0.9345403899721448\n"
     ]
    }
   ],
   "source": [
    "hard_late_fusion({'kernel': 'poly', 'degree': 3, 'random_state': 42, 'class_weight': 'balanced'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF confusion matrix\n",
      "[[785  22]\n",
      " [ 26 170]]\n",
      "svm confusion matrix\n",
      "[[759  48]\n",
      " [146  50]]\n",
      "(197, 2) (197, 2) (197,)\n",
      "197 data are predicted using combined results of 2 modalities\n",
      "(169, 2) (169, 2) (169,)\n",
      "169 data are predicted using combined results of 2 modalities\n",
      "(218, 2) (218, 2) (218,)\n",
      "218 data are predicted using combined results of 2 modalities\n",
      "(181, 2) (181, 2) (181,)\n",
      "181 data are predicted using combined results of 2 modalities\n",
      "(276, 2) (276, 2) (276,)\n",
      "276 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(185, 2) (185, 2) (185,)\n",
      "185 data are predicted using combined results of 2 modalities\n",
      "(230, 2) (230, 2) (230,)\n",
      "230 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(267, 2) (267, 2) (267,)\n",
      "267 data are predicted using combined results of 2 modalities\n",
      "(254, 2) (254, 2) (254,)\n",
      "254 data are predicted using combined results of 2 modalities\n",
      "(345, 2) (345, 2) (345,)\n",
      "345 data are predicted using combined results of 2 modalities\n",
      "Accuracy(protective): 0.15060240963855423 25 166\n",
      "Accuracy(non-protective): 0.9785661492978566 2648 2706\n",
      "F1-score: 0.20080321285140565\n",
      "Overall accuracy: 0.9307103064066853\n"
     ]
    }
   ],
   "source": [
    "hard_late_fusion({'kernel': 'poly', 'degree': 4, 'random_state': 42, 'class_weight': 'balanced'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF confusion matrix\n",
      "[[785  22]\n",
      " [ 26 170]]\n",
      "svm confusion matrix\n",
      "[[654 153]\n",
      " [ 86 110]]\n",
      "(197, 2) (197, 2) (197,)\n",
      "197 data are predicted using combined results of 2 modalities\n",
      "(169, 2) (169, 2) (169,)\n",
      "169 data are predicted using combined results of 2 modalities\n",
      "(218, 2) (218, 2) (218,)\n",
      "218 data are predicted using combined results of 2 modalities\n",
      "(181, 2) (181, 2) (181,)\n",
      "181 data are predicted using combined results of 2 modalities\n",
      "(276, 2) (276, 2) (276,)\n",
      "276 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(185, 2) (185, 2) (185,)\n",
      "185 data are predicted using combined results of 2 modalities\n",
      "(230, 2) (230, 2) (230,)\n",
      "230 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(267, 2) (267, 2) (267,)\n",
      "267 data are predicted using combined results of 2 modalities\n",
      "(254, 2) (254, 2) (254,)\n",
      "254 data are predicted using combined results of 2 modalities\n",
      "(345, 2) (345, 2) (345,)\n",
      "345 data are predicted using combined results of 2 modalities\n",
      "Accuracy(protective): 0.28313253012048195 47 166\n",
      "Accuracy(non-protective): 0.9349593495934959 2530 2706\n",
      "F1-score: 0.2416452442159383\n",
      "Overall accuracy: 0.8972841225626741\n"
     ]
    }
   ],
   "source": [
    "hard_late_fusion({'kernel': 'rbf', 'gamma': 'scale', 'random_state': 42, 'class_weight': 'balanced'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF confusion matrix\n",
      "[[785  22]\n",
      " [ 26 170]]\n",
      "svm confusion matrix\n",
      "[[712  95]\n",
      " [138  58]]\n",
      "(197, 2) (197, 2) (197,)\n",
      "197 data are predicted using combined results of 2 modalities\n",
      "(169, 2) (169, 2) (169,)\n",
      "169 data are predicted using combined results of 2 modalities\n",
      "(218, 2) (218, 2) (218,)\n",
      "218 data are predicted using combined results of 2 modalities\n",
      "(181, 2) (181, 2) (181,)\n",
      "181 data are predicted using combined results of 2 modalities\n",
      "(276, 2) (276, 2) (276,)\n",
      "276 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(185, 2) (185, 2) (185,)\n",
      "185 data are predicted using combined results of 2 modalities\n",
      "(230, 2) (230, 2) (230,)\n",
      "230 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(267, 2) (267, 2) (267,)\n",
      "267 data are predicted using combined results of 2 modalities\n",
      "(254, 2) (254, 2) (254,)\n",
      "254 data are predicted using combined results of 2 modalities\n",
      "(345, 2) (345, 2) (345,)\n",
      "345 data are predicted using combined results of 2 modalities\n",
      "Accuracy(protective): 0.28313253012048195 47 166\n",
      "Accuracy(non-protective): 0.9726533628972653 2632 2706\n",
      "F1-score: 0.3275261324041812\n",
      "Overall accuracy: 0.932799442896936\n"
     ]
    }
   ],
   "source": [
    "hard_late_fusion({'kernel': 'rbf', 'gamma': 'auto', 'random_state': 42, 'class_weight': 'balanced'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF confusion matrix\n",
      "[[785  22]\n",
      " [ 26 170]]\n",
      "svm confusion matrix\n",
      "[[762  45]\n",
      " [166  30]]\n",
      "(197, 2) (197, 2) (197,)\n",
      "197 data are predicted using combined results of 2 modalities\n",
      "(169, 2) (169, 2) (169,)\n",
      "169 data are predicted using combined results of 2 modalities\n",
      "(218, 2) (218, 2) (218,)\n",
      "218 data are predicted using combined results of 2 modalities\n",
      "(181, 2) (181, 2) (181,)\n",
      "181 data are predicted using combined results of 2 modalities\n",
      "(276, 2) (276, 2) (276,)\n",
      "276 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(185, 2) (185, 2) (185,)\n",
      "185 data are predicted using combined results of 2 modalities\n",
      "(230, 2) (230, 2) (230,)\n",
      "230 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(267, 2) (267, 2) (267,)\n",
      "267 data are predicted using combined results of 2 modalities\n",
      "(254, 2) (254, 2) (254,)\n",
      "254 data are predicted using combined results of 2 modalities\n",
      "(345, 2) (345, 2) (345,)\n",
      "345 data are predicted using combined results of 2 modalities\n",
      "Accuracy(protective): 0.19879518072289157 33 166\n",
      "Accuracy(non-protective): 0.9859571322985957 2668 2706\n",
      "F1-score: 0.27848101265822783\n",
      "Overall accuracy: 0.9404596100278552\n"
     ]
    }
   ],
   "source": [
    "hard_late_fusion({'kernel': 'rbf', 'C': 0.1, 'gamma': 'auto', 'random_state': 42, 'class_weight': 'balanced'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF confusion matrix\n",
      "[[785  22]\n",
      " [ 26 170]]\n",
      "svm confusion matrix\n",
      "[[703 104]\n",
      " [135  61]]\n",
      "(197, 2) (197, 2) (197,)\n",
      "197 data are predicted using combined results of 2 modalities\n",
      "(169, 2) (169, 2) (169,)\n",
      "169 data are predicted using combined results of 2 modalities\n",
      "(218, 2) (218, 2) (218,)\n",
      "218 data are predicted using combined results of 2 modalities\n",
      "(181, 2) (181, 2) (181,)\n",
      "181 data are predicted using combined results of 2 modalities\n",
      "(276, 2) (276, 2) (276,)\n",
      "276 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(185, 2) (185, 2) (185,)\n",
      "185 data are predicted using combined results of 2 modalities\n",
      "(230, 2) (230, 2) (230,)\n",
      "230 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(267, 2) (267, 2) (267,)\n",
      "267 data are predicted using combined results of 2 modalities\n",
      "(254, 2) (254, 2) (254,)\n",
      "254 data are predicted using combined results of 2 modalities\n",
      "(345, 2) (345, 2) (345,)\n",
      "345 data are predicted using combined results of 2 modalities\n",
      "Accuracy(protective): 0.29518072289156627 49 166\n",
      "Accuracy(non-protective): 0.9611973392461197 2601 2706\n",
      "F1-score: 0.30624999999999997\n",
      "Overall accuracy: 0.9227019498607242\n"
     ]
    }
   ],
   "source": [
    "hard_late_fusion({'kernel': 'rbf', 'C': 10.0, 'gamma': 'auto', 'random_state': 42, 'class_weight': 'balanced'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF confusion matrix\n",
      "[[785  22]\n",
      " [ 26 170]]\n",
      "svm confusion matrix\n",
      "[[657 150]\n",
      " [104  92]]\n",
      "(197, 2) (197, 2) (197,)\n",
      "197 data are predicted using combined results of 2 modalities\n",
      "(169, 2) (169, 2) (169,)\n",
      "169 data are predicted using combined results of 2 modalities\n",
      "(218, 2) (218, 2) (218,)\n",
      "218 data are predicted using combined results of 2 modalities\n",
      "(181, 2) (181, 2) (181,)\n",
      "181 data are predicted using combined results of 2 modalities\n",
      "(276, 2) (276, 2) (276,)\n",
      "276 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(185, 2) (185, 2) (185,)\n",
      "185 data are predicted using combined results of 2 modalities\n",
      "(230, 2) (230, 2) (230,)\n",
      "230 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(267, 2) (267, 2) (267,)\n",
      "267 data are predicted using combined results of 2 modalities\n",
      "(254, 2) (254, 2) (254,)\n",
      "254 data are predicted using combined results of 2 modalities\n",
      "(345, 2) (345, 2) (345,)\n",
      "345 data are predicted using combined results of 2 modalities\n",
      "Accuracy(protective): 0.3132530120481928 52 166\n",
      "Accuracy(non-protective): 0.9331116038433112 2525 2706\n",
      "F1-score: 0.26065162907268175\n",
      "Overall accuracy: 0.8972841225626741\n"
     ]
    }
   ],
   "source": [
    "hard_late_fusion({'kernel': 'rbf', 'C': 0.1, 'gamma': 'scale', 'random_state': 42, 'class_weight': 'balanced'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF confusion matrix\n",
      "[[785  22]\n",
      " [ 26 170]]\n",
      "svm confusion matrix\n",
      "[[683 124]\n",
      " [108  88]]\n",
      "(197, 2) (197, 2) (197,)\n",
      "197 data are predicted using combined results of 2 modalities\n",
      "(169, 2) (169, 2) (169,)\n",
      "169 data are predicted using combined results of 2 modalities\n",
      "(218, 2) (218, 2) (218,)\n",
      "218 data are predicted using combined results of 2 modalities\n",
      "(181, 2) (181, 2) (181,)\n",
      "181 data are predicted using combined results of 2 modalities\n",
      "(276, 2) (276, 2) (276,)\n",
      "276 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(185, 2) (185, 2) (185,)\n",
      "185 data are predicted using combined results of 2 modalities\n",
      "(230, 2) (230, 2) (230,)\n",
      "230 data are predicted using combined results of 2 modalities\n",
      "(275, 2) (275, 2) (275,)\n",
      "275 data are predicted using combined results of 2 modalities\n",
      "(267, 2) (267, 2) (267,)\n",
      "267 data are predicted using combined results of 2 modalities\n",
      "(254, 2) (254, 2) (254,)\n",
      "254 data are predicted using combined results of 2 modalities\n",
      "(345, 2) (345, 2) (345,)\n",
      "345 data are predicted using combined results of 2 modalities\n",
      "Accuracy(protective): 0.21084337349397592 35 166\n",
      "Accuracy(non-protective): 0.9571322985957132 2590 2706\n",
      "F1-score: 0.22082018927444794\n",
      "Overall accuracy: 0.9139972144846796\n"
     ]
    }
   ],
   "source": [
    "hard_late_fusion({'kernel': 'rbf', 'C': 10.0, 'gamma': 'scale', 'random_state': 42, 'class_weight': 'balanced'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
